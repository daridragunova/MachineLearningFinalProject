{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxtNes4y72qU"
      },
      "source": [
        "# Def Section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "qZfDYyz_8Chd",
        "outputId": "4054973a-8800-49e8-bdad-8fb40741d3fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'# Feature 20 extraction - \"Jaro Winkler Distance\" #'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "from pandas.core.indexes.base import trim_front\n",
        "#import pandas as pd\n",
        "\n",
        "\n",
        "\"\"\"# Cleaning broken instances #\"\"\"\n",
        "def original_data_cleaning(listofInstance,ctag1,ctag2,wtag):\n",
        "    \n",
        "    indx = 0\n",
        "    for i in listofInstance[ctag1]:\n",
        "        if  wtag in i:\n",
        "            listofInstance = listofInstance.drop(listofInstance.index[indx])\n",
        "        else:\n",
        "            indx = indx + 1\n",
        "        \n",
        "    listofInstance = listofInstance.loc[(listofInstance[ctag2].isnull() == False)]\n",
        "    listofInstance = listofInstance.reset_index(drop=True)\n",
        "    \n",
        "    return listofInstance\n",
        "\n",
        "    \n",
        "\"\"\"# Cleaning broken instances Test#\"\"\"\n",
        "def original_data_cleaning_test(listofInstance,ctag1,ctag2,wtag):\n",
        "    \n",
        "    indx = 0\n",
        "\n",
        "    for i in listofInstance[ctag1]:\n",
        "      if wtag in i:\n",
        "          listofInstance = listofInstance.drop(listofInstance.index[indx])\n",
        "      else:\n",
        "          indx = indx + 1\n",
        "    listofInstance = listofInstance.loc[(listofInstance[ctag2].isnull() == False)]\n",
        "    listofInstance = listofInstance.reset_index(drop=True)\n",
        "\n",
        "    return listofInstance\n",
        "\n",
        "\n",
        "\"\"\"# Cleaning sentences #\"\"\"\n",
        "\n",
        "#Pre-process sentences\n",
        "import nltk\n",
        "import string\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('words')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "words = set(nltk.corpus.words.words())\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def Preprocess_listofSentence(listofSentence):\n",
        " preprocess_list = []\n",
        " for sentence in listofSentence :\n",
        "  sentence_w_punct = \"\".join([i.lower() for i in sentence if i not in string.punctuation])\n",
        "\n",
        "  #lowercase\n",
        "  tokenize_sentence = nltk.tokenize.word_tokenize(sentence_w_punct.lower())\n",
        "  #tokenize\n",
        "  tokenize_sentence = nltk.tokenize.word_tokenize(sentence_w_punct)\n",
        "  # remove stopwords\n",
        "  words_w_stopwords = [i for i in tokenize_sentence if i not in stopwords]      \n",
        "  # lemmatize\n",
        "  words_lemmatize = (lemmatizer.lemmatize(w) for w in words_w_stopwords)         \n",
        "  #put the words together in a list\n",
        "  sentence_clean = ' '.join(w for w in words_lemmatize if w.lower() in words or w.isalpha())\n",
        "\n",
        "  preprocess_list.append(sentence_clean)\n",
        "\n",
        " return preprocess_list\n",
        "\n",
        "\n",
        "\"\"\"# Feature 1 extraction - \"word_count_dif\", the difference between number of words in sentence1 vs. sentence2 #\"\"\"\n",
        "\n",
        "def FeatureExtraction_WordCount(listofSentence1,listofSentence2):\n",
        "\n",
        "    words_count_dif = []\n",
        "    for i in range(len(listofSentence1)):\n",
        "        words_count_dif.append(abs(len(listofSentence1[i])-len(listofSentence2[i]))/(len(listofSentence1[i])+len(listofSentence2[i])))\n",
        " \n",
        "    return words_count_dif\n",
        "\n",
        "\n",
        "\"\"\"# Feature 2 extraction - \"word_overlap\", number of the same words in sentence1 and sentence2 #\"\"\"\n",
        "def FeatureExtraction_WordOverlap(listofSentence1,listofSentence2):\n",
        "\n",
        "    same_words = []\n",
        "    for i in range(len(listofSentence1)):\n",
        "        same_words.append(len(list(set(listofSentence1[i])&set(listofSentence2[i])))/(len(listofSentence1[i])+len(listofSentence2[i]))) \n",
        "    \n",
        "    return same_words\n",
        "\n",
        "\n",
        "\"\"\"# Feature 3 extraction - \"word_union\", number of unique words in sentence1 and sentence2 #\"\"\"\n",
        "def FeatureExtraction_WordUnion(listofSentence1,listofSentence2):\n",
        "\n",
        "    word_union = [] \n",
        "    for i in range(len(listofSentence1)):\n",
        "        word_union.append(len(list(set(listofSentence1[i]) | set(listofSentence2[i])))/(len(listofSentence1[i])+len(listofSentence2[i])))\n",
        "\n",
        "    return word_union\n",
        "\n",
        "\n",
        "\"\"\"# Feature 4 extraction - \"three_gram_overlap\", number of same sequence of three words in sentence1 and sentence2 #\"\"\"\n",
        "#import re\n",
        "from nltk.util import ngrams\n",
        "\n",
        "def FeatureExtraction_3gramOverlap(listofSentence1,listofSentence2):\n",
        "    \n",
        "    tokenize_sentence1 = []\n",
        "    tokenize_sentence2 = []\n",
        "    for i in range(len(listofSentence1)):\n",
        "      tokenize_sentence1.append(nltk.tokenize.word_tokenize(listofSentence1[i]))\n",
        "      tokenize_sentence2.append(nltk.tokenize.word_tokenize(listofSentence2[i]))\n",
        "    \n",
        "    three_gram_sen1 = []\n",
        "    three_gram_sen2 = []\n",
        "    for i in range(len(listofSentence1)):\n",
        "        three_gram_sen1.append(list(ngrams(tokenize_sentence1[i], 3)))\n",
        "        three_gram_sen2.append(list(ngrams(tokenize_sentence2[i], 3)))\n",
        "\n",
        "    same_ngrams = []\n",
        "    for i in range(len(listofSentence1)):\n",
        "        same_ngrams.append(len(list(set(three_gram_sen1[i]) & set(three_gram_sen2[i]))) /(len(set(three_gram_sen1[i]))+len(set(three_gram_sen2[i]))))\n",
        "    \n",
        "    return same_ngrams\n",
        "\n",
        "\n",
        "\"\"\"# Feature 5 extraction - \"three_gram_union\" #\"\"\"\n",
        "def FeatureExtraction_3gramUnion(listofSentence1,listofSentence2):\n",
        "\n",
        "    tokenize_sentence1 = []\n",
        "    tokenize_sentence2 = []\n",
        "    for i in range(len(listofSentence1)):\n",
        "      tokenize_sentence1.append(nltk.tokenize.word_tokenize(listofSentence1[i]))\n",
        "      tokenize_sentence2.append(nltk.tokenize.word_tokenize(listofSentence2[i]))\n",
        "\n",
        "    three_gram_sen1 = []\n",
        "    three_gram_sen2 = []\n",
        "    for i in range(len(listofSentence1)):\n",
        "        three_gram_sen1.append(list(ngrams(listofSentence1[i], 3)))\n",
        "        three_gram_sen2.append(list(ngrams(listofSentence2[i], 3)))\n",
        "\n",
        "    union_ngrams = []\n",
        "    for i in range(len(listofSentence1)):\n",
        "        union_ngrams.append(len(list(set(three_gram_sen1[i]) | set(three_gram_sen2[i])))/(len(set(three_gram_sen1[i]))+len(set(three_gram_sen2[i]))))\n",
        "\n",
        "    return union_ngrams\n",
        "\n",
        "\n",
        "\"\"\"# Feature 6 extraction - \"two_gram_overlap\" #\"\"\"\n",
        "def FeatureExtraction_2gramOverlap(listofSentence1,listofSentence2):\n",
        "    tokenize_sentence1 = []\n",
        "    tokenize_sentence2 = []\n",
        "    for i in range(len(listofSentence1)):\n",
        "      tokenize_sentence1.append(nltk.tokenize.word_tokenize(listofSentence1[i]))\n",
        "      tokenize_sentence2.append(nltk.tokenize.word_tokenize(listofSentence2[i]))\n",
        "\n",
        "    two_gram_sen1 = []\n",
        "    two_gram_sen2 = []\n",
        "    for i in range(len(listofSentence1)):\n",
        "        two_gram_sen1.append(list(ngrams(listofSentence1[i], 2)))\n",
        "        two_gram_sen2.append(list(ngrams(listofSentence2[i], 2)))\n",
        "\n",
        "    same_ngrams = []\n",
        "    for i in range(len(listofSentence1)):\n",
        "        same_ngrams.append(len(list(set(two_gram_sen1[i]) & set(two_gram_sen2[i])))/(len(set(two_gram_sen1[i]))+len(set(two_gram_sen2[i]))))\n",
        "    \n",
        "    return same_ngrams\n",
        "\n",
        "\n",
        "\"\"\"# Feature 7 extraction - \"two_gram_union\" #\"\"\"\n",
        "def FeatureExtraction_2gramUnion(listofSentence1,listofSentence2):\n",
        "    tokenize_sentence1 = []\n",
        "    tokenize_sentence2 = []\n",
        "    for i in range(len(listofSentence1)):\n",
        "      tokenize_sentence1.append(nltk.tokenize.word_tokenize(listofSentence1[i]))\n",
        "      tokenize_sentence2.append(nltk.tokenize.word_tokenize(listofSentence2[i]))\n",
        "      \n",
        "    two_gram_sen1 = []\n",
        "    two_gram_sen2 = []\n",
        "    for i in range(len(listofSentence1)):\n",
        "        two_gram_sen1.append(list(ngrams(listofSentence1[i], 2)))\n",
        "        two_gram_sen2.append(list(ngrams(listofSentence2[i], 2)))\n",
        "\n",
        "    union_ngrams = []\n",
        "    for i in range(len(listofSentence1)):\n",
        "        union_ngrams.append(len(list(set(two_gram_sen1[i]) | set(two_gram_sen2[i])))/(len(set(two_gram_sen1[i]))+len(set(two_gram_sen2[i]))))\n",
        "\n",
        "    return union_ngrams\n",
        "\n",
        "\n",
        "\"\"\"# Feature 8 extraction - \"num_overlap\", number of the same numbers in sentence1 and sentence2 #\"\"\"\n",
        "import re\n",
        "def FeatureExtraction_NumOverlap(listofSentence1,listofSentence2):\n",
        "\n",
        "# step 1 - extract numbers including float and persentage\n",
        "# creating a patern for step 1 \n",
        "    numeric_const_pattern = '[-+]? (?: (?: \\d* \\. \\d+ ) | (?: \\d+ \\.? ) )(?: [Ee] [+-]? \\d+ ) ?' \n",
        "# applying filter to create a list of found numbers\n",
        "    num_filter_patern = re.compile(numeric_const_pattern, re.VERBOSE)\n",
        "\n",
        "# step 2 - count how many numbers are matching    \n",
        "    same_nums = []\n",
        "    for i in range(len(listofSentence1)):\n",
        "        total_num_temp = len(list(set(num_filter_patern.findall(listofSentence1[i])) | set(num_filter_patern.findall(listofSentence2[i]))))\n",
        "        if total_num_temp == 0:\n",
        "          same_nums.append(float(0))\n",
        "        else:\n",
        "          same_num_temp = len(list(set(num_filter_patern.findall(listofSentence1[i])) & set(num_filter_patern.findall(listofSentence2[i])))) \n",
        "          same_nums.append(float(same_num_temp/total_num_temp))\n",
        "#        same_nums.append(len(list(set(num_filter_patern.findall(listofSentence1[i])) & set(num_filter_patern.findall(listofSentence2[i])))))\n",
        "    \n",
        "    return same_nums\n",
        "\n",
        "\n",
        "\"\"\"# Feature 9 extraction - \"num_capitalized_letters\" #\"\"\" \n",
        "def FeatureExtraction_CapitalLetters(listofSentence1,listofSentence2):\n",
        "    import re\n",
        "    import nltk\n",
        "    nltk.download('stopwords')\n",
        "\n",
        "    stopwords = nltk.corpus.stopwords.words('english')\n",
        "    stopwords = [each_string.capitalize() for each_string in stopwords]\n",
        "\n",
        "    cap_words_sen1 = []\n",
        "    for i in range(len(listofSentence1)):\n",
        "      res1 = re.findall('([A-Z][a-z]+)', listofSentence1[i])\n",
        "      words_w_stopwords = [i for i in res1 if i not in stopwords] \n",
        "      cap_words_sen1.append(words_w_stopwords)\n",
        "    \n",
        "    cap_words_sen2 = []\n",
        "    for i in range(len(listofSentence2)):\n",
        "      res2 = re.findall('([A-Z][a-z]+)', listofSentence2[i])\n",
        "      words_w_stopwords = [i for i in res2 if i not in stopwords] \n",
        "      cap_words_sen2.append(words_w_stopwords)\n",
        "\n",
        "    num_capitalized_letters = []\n",
        "    for i in range(len(listofSentence1)):\n",
        "      total_num_temp = len(list(set(cap_words_sen1[i]) | set(cap_words_sen2[i])))\n",
        "      if total_num_temp == 0:\n",
        "        num_capitalized_letters.append(float(0))\n",
        "      else:\n",
        "        num_capitalized_letters.append(float(len(list(set(cap_words_sen1[i]) & set(cap_words_sen2[i])))/total_num_temp))\n",
        "\n",
        "    return num_capitalized_letters\n",
        "\n",
        "\"\"\"# Feature 10 extraction - \"cosine_sim\" #\"\"\" \n",
        "def FeatureExtraction_CosineSim(listofSentence1,listofSentence2):\n",
        "    #turn sentences into vectors\n",
        "    from collections import Counter\n",
        "    import math\n",
        "    import re \n",
        "\n",
        "    WORD = re.compile(r\"\\w+\")\n",
        "\n",
        "    def text_to_vector(text):\n",
        "      words = WORD.findall(text)\n",
        "      return Counter(words)\n",
        "\n",
        "    cos_score = []\n",
        "    for i in range(len(listofSentence1)):\n",
        "      vector1 = text_to_vector(listofSentence1[i])\n",
        "      vector2 = text_to_vector(listofSentence2[i])\n",
        "\n",
        "      intersection = set(vector1.keys()) & set(vector2.keys())\n",
        "      numerator = sum([vector1[x] * vector2[x] for x in intersection])\n",
        "\n",
        "      sum1 = sum([vector1[x] ** 2 for x in list(vector1.keys())])\n",
        "      sum2 = sum([vector2[x] ** 2 for x in list(vector2.keys())])\n",
        "      denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
        "\n",
        "      if not denominator:\n",
        "          cosine = 0.0\n",
        "      else:\n",
        "          cosine = float(numerator) / denominator\n",
        "\n",
        "      cos_score.append(cosine)\n",
        "\n",
        "    return cos_score\n",
        "\n",
        "\"\"\"# Feature 11 extraction - \"BLEU-4\" #\"\"\" \n",
        "#https://stackoverflow.com/questions/32395880/calculate-bleu-score-in-python\n",
        "\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "def FeatureExtraction_Bleu4(listofSentence1, listofSentence2):\n",
        "  tokenize_sentence1 = []\n",
        "  tokenize_sentence2 = []\n",
        "  for i in range(len(listofSentence1)):\n",
        "    tokenize_sentence1.append(nltk.tokenize.word_tokenize(listofSentence1[i]))\n",
        "    tokenize_sentence2.append(nltk.tokenize.word_tokenize(listofSentence2[i]))\n",
        "\n",
        "  bleu_score = []\n",
        "  for i in range(len(listofSentence1)):\n",
        "    bleu_score.append(nltk.translate.bleu_score.sentence_bleu(tokenize_sentence1[i], tokenize_sentence2[i]))\n",
        "  return bleu_score\n",
        "\n",
        "\"\"\"# Feature 12 extraction - \"BLEU-2\" #\"\"\" \n",
        "def FeatureExtraction_Bleu2(listofSentence1, listofSentence2):\n",
        "  tokenize_sentence1 = []\n",
        "  tokenize_sentence2 = []\n",
        "  for i in range(len(listofSentence1)):\n",
        "    tokenize_sentence1.append(nltk.tokenize.word_tokenize(listofSentence1[i]))\n",
        "    tokenize_sentence2.append(nltk.tokenize.word_tokenize(listofSentence2[i]))\n",
        "\n",
        "  bleu_score = []\n",
        "  for i in range(len(listofSentence1)):\n",
        "    bleu_score.append(nltk.translate.bleu_score.sentence_bleu(tokenize_sentence1[i], tokenize_sentence2[i], weights = (0.5, 0.5)))\n",
        "  return bleu_score\n",
        "\n",
        "\"\"\"# Feature 13 extraction - \"BLEU-3\" #\"\"\" \n",
        "def FeatureExtraction_Bleu3(listofSentence1, listofSentence2):\n",
        "  tokenize_sentence1 = []\n",
        "  tokenize_sentence2 = []\n",
        "  for i in range(len(listofSentence1)):\n",
        "    tokenize_sentence1.append(nltk.tokenize.word_tokenize(listofSentence1[i]))\n",
        "    tokenize_sentence2.append(nltk.tokenize.word_tokenize(listofSentence2[i]))\n",
        "\n",
        "  bleu_score = []\n",
        "  for i in range(len(listofSentence1)):\n",
        "    bleu_score.append(nltk.translate.bleu_score.sentence_bleu(tokenize_sentence1[i], tokenize_sentence2[i], weights = (0.3, 0.3, 0.3)))\n",
        "  return bleu_score\n",
        "\n",
        "\"\"\"# Feature 14 extraction - \"NIST1\" #\"\"\" \n",
        "def FeatureExtraction_NIST1(listofSentence1, listofSentence2):\n",
        "  tokenize_sentence1 = []\n",
        "  tokenize_sentence2 = []\n",
        "  for i in range(len(listofSentence1)):\n",
        "    tokenize_sentence1.append(nltk.tokenize.word_tokenize(listofSentence1[i]))\n",
        "    tokenize_sentence2.append(nltk.tokenize.word_tokenize(listofSentence2[i]))\n",
        "\n",
        "  score = []\n",
        "  for s in range(len(listofSentence1)):\n",
        "    score.append(nltk.translate.nist_score.sentence_nist(tokenize_sentence1[s], tokenize_sentence2[s], n=1))\n",
        "\n",
        "  return score\n",
        "\n",
        "\n",
        "\"\"\"# Feature 15 extraction - \"NIST2\" #\"\"\" \n",
        "def FeatureExtraction_NIST2(listofSentence1, listofSentence2):\n",
        "  tokenize_sentence1 = []\n",
        "  tokenize_sentence2 = []\n",
        "  for i in range(len(listofSentence1)):\n",
        "    tokenize_sentence1.append(nltk.tokenize.word_tokenize(listofSentence1[i]))\n",
        "    tokenize_sentence2.append(nltk.tokenize.word_tokenize(listofSentence2[i]))\n",
        "\n",
        "  score = []\n",
        "  for s in range(len(listofSentence1)):\n",
        "    score.append(nltk.translate.nist_score.sentence_nist(tokenize_sentence1[s], tokenize_sentence2[s], n=2))\n",
        "\n",
        "  return score\n",
        "\n",
        "\n",
        "\"\"\"# Feature 16 extraction - \"NIST3\" #\"\"\" \n",
        "def FeatureExtraction_NIST3(listofSentence1, listofSentence2):\n",
        "  tokenize_sentence1 = []\n",
        "  tokenize_sentence2 = []\n",
        "  for i in range(len(listofSentence1)):\n",
        "    tokenize_sentence1.append(nltk.tokenize.word_tokenize(listofSentence1[i]))\n",
        "    tokenize_sentence2.append(nltk.tokenize.word_tokenize(listofSentence2[i]))\n",
        "\n",
        "  score = []\n",
        "  for s in range(len(listofSentence1)):\n",
        "    score.append(nltk.translate.nist_score.sentence_nist(tokenize_sentence1[s], tokenize_sentence2[s], n=3))\n",
        "\n",
        "  return score\n",
        "\n",
        "\n",
        "\"\"\"# Feature 17 extraction - \"TER\" #\"\"\" #NOT IMPLEMENTED YET\n",
        "def FeatureExtraction_NIST1(listofSentence1, listofSentence2):\n",
        "  tokenize_sentence1 = []\n",
        "  tokenize_sentence2 = []\n",
        "  for i in range(len(listofSentence1)):\n",
        "    tokenize_sentence1.append(nltk.tokenize.word_tokenize(listofSentence1[i]))\n",
        "    tokenize_sentence2.append(nltk.tokenize.word_tokenize(listofSentence2[i]))\n",
        "\n",
        "  for s in range(len(listofSentence1)):\n",
        "    nltk.translate.nist_score.sentence_nist(tokenize_sentence1[s], tokenize_sentence2[s], n=1)\n",
        "\n",
        "\n",
        "\"\"\"# Feature 18 extraction - \"METEOR\" #\"\"\" \n",
        "from nltk.translate import meteor\n",
        "def FeatureExtraction_Meteor(listofSentence1, listofSentence2):\n",
        "  tokenize_sentence1 = []\n",
        "  tokenize_sentence2 = []\n",
        "  for i in range(len(listofSentence1)):\n",
        "    tokenize_sentence1.append(nltk.tokenize.word_tokenize(listofSentence1[i]))\n",
        "    tokenize_sentence2.append(nltk.tokenize.word_tokenize(listofSentence2[i]))\n",
        "\n",
        "  score = []\n",
        "  for i in range(len(listofSentence1)):\n",
        "    score.append(meteor([tokenize_sentence1[i]], tokenize_sentence2[i]))\n",
        "  \n",
        "  return score\n",
        "\n",
        "\n",
        "\"\"\"# Feature 19 extraction - \"Levenshtein Distance\" #\"\"\" \n",
        "#https://towardsdatascience.com/text-similarity-w-levenshtein-distance-in-python-2f7478986e75\n",
        "from difflib import ndiff\n",
        "import pylev\n",
        "def FeatureExtraction_LD(listofSentence1, listofSentence2):\n",
        "    ld = []\n",
        "    for i in range(len(listofSentence1)):\n",
        "      ld.append(pylev.levenshtein(listofSentence1[i],listofSentence2[i]))\n",
        "    return ld\n",
        "\n",
        "\"\"\"# Feature 20 extraction - \"Jaro Winkler Distance\" #\"\"\" \n",
        "#import jaro\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BwEMzWgzlzcZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EViscUIeohKS"
      },
      "source": [
        "# File reading section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-h7WzsoCFkD",
        "outputId": "6ce22ebd-a59e-4083-e062-43e0478fd9fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          id                                          sentence1  \\\n",
            "0  test_id_0  Jews , Americans and their allies had \" evil \"...   \n",
            "1  test_id_1  Zuccarini was ordered held without bail Wednes...   \n",
            "2  test_id_2  Contribute 2 costs 69 for an individual licens...   \n",
            "3  test_id_3  The gunman , 26-year-old Harold Kilpatrick jnr...   \n",
            "4  test_id_4   The bank requires growth from elsewhere in th...   \n",
            "\n",
            "                                           sentence2  gold_label  \n",
            "0  . said Amrozi , Indonesia like nations colonis...         NaN  \n",
            "1  . Fla , Lauderdale Fort in judge federal a by ...         NaN  \n",
            "2  The US version will cost $ 99 for an individua...         NaN  \n",
            "3  The gunman , identified as Harold Kilpatrick J...         NaN  \n",
            "4  In \" Open Range , \" Costner plays a cowboy who...         NaN  \n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3998 entries, 0 to 3997\n",
            "Data columns (total 4 columns):\n",
            " #   Column      Non-Null Count  Dtype  \n",
            "---  ------      --------------  -----  \n",
            " 0   id          3998 non-null   object \n",
            " 1   sentence1   3998 non-null   object \n",
            " 2   sentence2   3998 non-null   object \n",
            " 3   gold_label  0 non-null      float64\n",
            "dtypes: float64(1), object(3)\n",
            "memory usage: 125.1+ KB\n"
          ]
        }
      ],
      "source": [
        "# Open files and read data\n",
        "import pandas as pd\n",
        "#from midtermlib import *\n",
        "\n",
        "LABELS = [\"id\", \"sentence1\", \"sentence2\", \"gold_label\"]\n",
        "train_data = pd.read_csv(\"train_with_label.csv\", delimiter=',', names= LABELS)\n",
        "dev_data = pd.read_csv(\"dev_with_label.csv\", delimiter=',', names= LABELS)\n",
        "test_data = pd.read_csv(\"test_without_label.csv\", delimiter=',', names= LABELS)\n",
        "\n",
        "\n",
        "train_data = original_data_cleaning(train_data,\"sentence2\",\"gold_label\",\"train_id_\")\n",
        "dev_data = original_data_cleaning(dev_data,\"sentence2\",\"gold_label\",\"dev_id_\")\n",
        "test_data = original_data_cleaning_test(test_data,\"sentence1\",\"sentence2\",\"test_id_\")\n",
        "\n",
        "print(test_data.head())\n",
        "test_data.info()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsNpvZxyTYaJ"
      },
      "source": [
        "# Pre-processing Section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IyWdZBwr5v89",
        "outputId": "3bbd045d-d14f-4043-8311-c204b218134b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Pre-processing complete \n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Pre-process sentences including tokenization\n",
        "# Cleaning sentences\n",
        "preprocess_sent1 = Preprocess_listofSentence(train_data['sentence1'])\n",
        "preprocess_sent2 = Preprocess_listofSentence(train_data['sentence2'])\n",
        "train_data['clean_sent1'] = preprocess_sent1\n",
        "train_data['clean_sent2'] = preprocess_sent2\n",
        "\n",
        "dev_preprocess_sent1 = Preprocess_listofSentence(dev_data['sentence1'])\n",
        "dev_preprocess_sent2 = Preprocess_listofSentence(dev_data['sentence2'])\n",
        "dev_data['clean_sent1'] = dev_preprocess_sent1\n",
        "dev_data['clean_sent2'] = dev_preprocess_sent2\n",
        "\n",
        "test_preprocess_sent1 = Preprocess_listofSentence(test_data['sentence1'])\n",
        "test_preprocess_sent2 = Preprocess_listofSentence(test_data['sentence2'])\n",
        "test_data['clean_sent1'] = test_preprocess_sent1\n",
        "test_data['clean_sent2'] = test_preprocess_sent2\n",
        "\n",
        "# Tokenize sentences \n",
        "train_data['tok_sentence1'] = train_data.apply(lambda row: nltk.word_tokenize(row['clean_sent1']), axis=1)\n",
        "train_data['tok_sentence2'] = train_data.apply(lambda row: nltk.word_tokenize(row['clean_sent2']), axis=1)\n",
        "\n",
        "dev_data['tok_sentence1'] = dev_data.apply(lambda row: nltk.word_tokenize(row['clean_sent1']), axis=1)\n",
        "dev_data['tok_sentence2'] = dev_data.apply(lambda row: nltk.word_tokenize(row['clean_sent2']), axis=1)\n",
        "\n",
        "test_data['tok_sentence1'] = test_data.apply(lambda row: nltk.word_tokenize(row['clean_sent1']), axis=1)\n",
        "test_data['tok_sentence2'] = test_data.apply(lambda row: nltk.word_tokenize(row['clean_sent2']), axis=1)\n",
        "\n",
        "print(' Pre-processing complete \\n')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3i6GJzew6Fm"
      },
      "source": [
        "# Features Extraction Section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H0cf2YlduwvK",
        "outputId": "41cb3829-a26f-4c4b-c10a-893f3c747b68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Feature 1 - word_count_dif extracted\n"
          ]
        }
      ],
      "source": [
        "# Feature 1 extraction - \"word_count_dif\", the difference between number of words in sentence1 vs. sentence2\n",
        "\n",
        "train_data['word_count_dif'] = FeatureExtraction_WordCount(train_data['tok_sentence1'],train_data['tok_sentence2'])\n",
        "dev_data['word_count_dif'] = FeatureExtraction_WordCount(dev_data['tok_sentence1'],dev_data['tok_sentence2'])\n",
        "test_data['word_count_dif'] = FeatureExtraction_WordCount(test_data['tok_sentence1'],test_data['tok_sentence2'])\n",
        "print('\\n Feature 1 - word_count_dif extracted')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RHmlDJWi1I7i",
        "outputId": "b5b6782a-9fd8-40e0-b036-7eab0d53079d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Feature 2 - word_overlap extracted\n"
          ]
        }
      ],
      "source": [
        "# Feature 2 extraction - \"word_overlap\", number of the same words in sentence1 and sentence2\n",
        "\n",
        "train_data['word_overlap'] = FeatureExtraction_WordOverlap(train_data['tok_sentence1'],train_data['tok_sentence2'])\n",
        "dev_data['word_overlap'] = FeatureExtraction_WordOverlap(dev_data['tok_sentence1'],dev_data['tok_sentence2'])\n",
        "test_data['word_overlap'] = FeatureExtraction_WordOverlap(test_data['tok_sentence1'],test_data['tok_sentence2'])\n",
        "print('\\n Feature 2 - word_overlap extracted')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bF4nxR71lMul",
        "outputId": "bfb6b105-e3c5-435f-aad8-648a81388394"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Feature 3 - word_union extracted\n"
          ]
        }
      ],
      "source": [
        "# Feature 3 extraction - \"word_union\", number of unique words in sentence1 and sentence2\n",
        "\n",
        "train_data['word_union'] = FeatureExtraction_WordUnion(train_data['tok_sentence1'],train_data['tok_sentence2'])\n",
        "dev_data['word_union'] = FeatureExtraction_WordUnion(dev_data['tok_sentence1'],dev_data['tok_sentence2'])\n",
        "test_data['word_union'] = FeatureExtraction_WordUnion(test_data['tok_sentence1'],test_data['tok_sentence2'])\n",
        "print('\\n Feature 3 - word_union extracted')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vv2ejXQBu5DC",
        "outputId": "4fe1aed4-ac27-4298-e5cb-5bbc6876c59c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Feature 4 - 3gram_overlap extracted\n"
          ]
        }
      ],
      "source": [
        "# Feature 4 extraction - \"three_gram_overlap\", number of same sequence of three words in sentence1 and sentence2\n",
        "\n",
        "train_data['3gram_overlap'] = FeatureExtraction_3gramOverlap(train_data['sentence1'],train_data['sentence2'])\n",
        "dev_data['3gram_overlap'] = FeatureExtraction_3gramOverlap(dev_data['sentence1'],dev_data['sentence2'])\n",
        "test_data['3gram_overlap'] = FeatureExtraction_3gramOverlap(test_data['sentence1'],test_data['sentence2'])\n",
        "print('\\n Feature 4 - 3gram_overlap extracted')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50fHAjoYcx5F",
        "outputId": "23e4a1e6-8766-439c-9154-3d68ae628d39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Feature 5 - 3gram_union extracted\n"
          ]
        }
      ],
      "source": [
        "# Feature 5 extraction - \"three_gram_union\" - something does not work, I switched it off\n",
        "train_data['3gram_union'] = FeatureExtraction_3gramUnion(train_data['sentence1'],train_data['sentence2'])\n",
        "dev_data['3gram_union'] = FeatureExtraction_3gramUnion(dev_data['sentence1'],dev_data['sentence2'])\n",
        "test_data['3gram_union'] = FeatureExtraction_3gramUnion(test_data['sentence1'],test_data['sentence2'])\n",
        "print('\\n Feature 5 - 3gram_union extracted')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XDsPkvQUiez9",
        "outputId": "6c46df94-bfb5-4c83-8ff9-3b3ecf119147"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Feature 6 - 2gram_overlap extracted\n"
          ]
        }
      ],
      "source": [
        "# Feature 6 extraction - \"two_gram_overlap\", number of same sequence of two words in sentence1 and sentence2\n",
        "train_data['2gram_overlap'] = FeatureExtraction_2gramOverlap(train_data['sentence1'],train_data['sentence2'])\n",
        "dev_data['2gram_overlap'] = FeatureExtraction_2gramOverlap(dev_data['sentence1'],dev_data['sentence2'])\n",
        "test_data['2gram_overlap'] = FeatureExtraction_2gramOverlap(test_data['sentence1'],test_data['sentence2'])\n",
        "print('\\n Feature 6 - 2gram_overlap extracted')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gypmeo4XkhMc",
        "outputId": "a5d63748-4290-4f85-8fd6-cf6dfc1159b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Feature 7 - 2gram_union extracted\n"
          ]
        }
      ],
      "source": [
        "# Feature 7 extraction - \"two_gram_union\", number of same sequence of two words in sentence1 and sentence2\n",
        "train_data['2gram_union'] = FeatureExtraction_2gramUnion(train_data['sentence1'],train_data['sentence2'])\n",
        "dev_data['2gram_union'] = FeatureExtraction_2gramUnion(dev_data['sentence1'],dev_data['sentence2'])\n",
        "test_data['2gram_union'] = FeatureExtraction_2gramUnion(test_data['sentence1'],test_data['sentence2'])\n",
        "print('\\n Feature 7 - 2gram_union extracted')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2Qtaus3RHlo",
        "outputId": "f196f6b1-9596-486c-d11b-d085ed5336a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Feature 8 - num_overlap extracted\n"
          ]
        }
      ],
      "source": [
        "# Feature 8 extraction - \"numbers_overlap\", the number of same numbers in sentence1 and sentence2\n",
        "train_data['num_overlap'] = FeatureExtraction_NumOverlap(train_data['sentence1'],train_data['sentence2'])\n",
        "dev_data['num_overlap'] = FeatureExtraction_NumOverlap(dev_data['sentence1'],dev_data['sentence2'])\n",
        "test_data['num_overlap'] = FeatureExtraction_NumOverlap(test_data['sentence1'],test_data['sentence2'])\n",
        "print('\\n Feature 8 - num_overlap extracted')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F5jwa9jtR9df",
        "outputId": "f8a095e2-af8e-4c57-cd33-7ebcfc27032b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Feature 9 - cap_words_overlap extracted\n"
          ]
        }
      ],
      "source": [
        "# Feature 9 extraction - \"cap_words_overlap\", the number of same words with first capital letter in sentence1 and sentence2\n",
        "train_data['cap_words_overlap'] = FeatureExtraction_CapitalLetters(train_data['sentence1'],train_data['sentence2'])\n",
        "dev_data['cap_words_overlap'] = FeatureExtraction_CapitalLetters(dev_data['sentence1'],dev_data['sentence2'])\n",
        "test_data['cap_words_overlap'] = FeatureExtraction_CapitalLetters(test_data['sentence1'],test_data['sentence2'])\n",
        "print('\\n Feature 9 - cap_words_overlap extracted')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5XSFXku0KMY",
        "outputId": "87c07d61-5300-4cef-e01b-82f8bf33e743"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Feature 10 - cosine_sim extracted\n"
          ]
        }
      ],
      "source": [
        "# Feature 10 extraction - \"cosine_sim\", similarity of two sentences\n",
        "train_data['cosine_sim'] = FeatureExtraction_CosineSim(train_data['sentence1'],train_data['sentence2'])\n",
        "dev_data['cosine_sim'] = FeatureExtraction_CosineSim(dev_data['sentence1'],dev_data['sentence2'])\n",
        "test_data['cosine_sim'] = FeatureExtraction_CosineSim(test_data['sentence1'],test_data['sentence2'])\n",
        "print('\\n Feature 10 - cosine_sim extracted')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q5yOQyZKjJm3",
        "outputId": "18e3411d-4f8a-4cc6-cde2-3b5354a74a18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.8/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.8/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Feature 11 - bleu4 extracted\n"
          ]
        }
      ],
      "source": [
        "# Feature 11 extraction - \"bleu4\", similarity of two sentences\n",
        "train_data['bleu4'] = FeatureExtraction_Bleu4(train_data['sentence1'], train_data['sentence2'])\n",
        "dev_data['bleu4'] = FeatureExtraction_Bleu4(dev_data['sentence1'],dev_data['sentence2'])\n",
        "test_data['bleu4'] = FeatureExtraction_Bleu4(test_data['sentence1'], test_data['sentence2'])\n",
        "print('\\n Feature 11 - bleu4 extracted')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j4JUs_VNHu4l",
        "outputId": "bf7069a5-b722-4b8d-de9d-a736908134e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Feature 12 - bleu2 extracted\n",
            "0       4.717069e-155\n",
            "1       6.266260e-155\n",
            "2       4.839611e-155\n",
            "3       4.797708e-155\n",
            "4       6.266260e-155\n",
            "            ...      \n",
            "3993    4.398689e-155\n",
            "3994    4.306075e-155\n",
            "3995    3.768634e-155\n",
            "3996    2.818988e-155\n",
            "3997    5.637976e-155\n",
            "Name: bleu2, Length: 3998, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# Feature 12 extraction - \"bleu2\", similarity of two sentences\n",
        "train_data['bleu2'] = FeatureExtraction_Bleu2(train_data['sentence1'], train_data['sentence2'])\n",
        "dev_data['bleu2'] = FeatureExtraction_Bleu2(dev_data['sentence1'],dev_data['sentence2'])\n",
        "test_data['bleu2'] = FeatureExtraction_Bleu2(test_data['sentence1'], test_data['sentence2'])\n",
        "print('\\n Feature 12 - bleu2 extracted')\n",
        "print(test_data['bleu2'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZXC5pESY75RH",
        "outputId": "e8ea202e-b3e5-4ffe-87cb-75457cce53e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Feature 13 - bleu3 extracted\n",
            "0       1.247351e-185\n",
            "1       1.230828e-185\n",
            "2       9.933134e-186\n",
            "3       1.173023e-185\n",
            "4       1.027403e-185\n",
            "            ...      \n",
            "3992    1.408693e-185\n",
            "3993    1.303436e-185\n",
            "3994    1.094642e-185\n",
            "3995    1.125401e-185\n",
            "3996    1.050501e-185\n",
            "Name: bleu3, Length: 3997, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# Feature 13 extraction - \"bleu3\", similarity of two sentences\n",
        "train_data['bleu3'] = FeatureExtraction_Bleu3(train_data['sentence1'], train_data['sentence2'])\n",
        "dev_data['bleu3'] = FeatureExtraction_Bleu3(dev_data['sentence1'],dev_data['sentence2'])\n",
        "test_data['bleu3'] = FeatureExtraction_Bleu3(test_data['sentence1'], test_data['sentence2'])\n",
        "print('\\n Feature 13 - bleu3 extracted')\n",
        "print(dev_data['bleu3'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dan3GzCo764i",
        "outputId": "1907b18c-8c16-4920-dacf-c5be48f58123"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Feature 14 - nist1 extracted\n",
            "0       None\n",
            "1       None\n",
            "2       None\n",
            "3       None\n",
            "4       None\n",
            "        ... \n",
            "3992    None\n",
            "3993    None\n",
            "3994    None\n",
            "3995    None\n",
            "3996    None\n",
            "Name: nist1, Length: 3997, dtype: object\n"
          ]
        }
      ],
      "source": [
        "# Feature 14 extraction - \"NIST1\", similarity of two sentences\n",
        "train_data['nist1'] = FeatureExtraction_NIST1(train_data['sentence1'], train_data['sentence2'])\n",
        "dev_data['nist1'] = FeatureExtraction_NIST1(dev_data['sentence1'],dev_data['sentence2'])\n",
        "test_data['nist1'] = FeatureExtraction_NIST1(test_data['sentence1'], test_data['sentence2'])\n",
        "print('\\n Feature 14 - nist1 extracted')\n",
        "print(dev_data['nist1'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k5iwT-J4785S",
        "outputId": "a4e9c723-f9aa-41d7-adaf-0ee4e21ee18b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Feature 15 - NIST2 extracted\n",
            "0       0.163180\n",
            "1       0.275646\n",
            "2       0.123285\n",
            "3       0.134915\n",
            "4       0.228099\n",
            "          ...   \n",
            "3992    0.317667\n",
            "3993    0.348143\n",
            "3994    0.196662\n",
            "3995    0.226528\n",
            "3996    0.164343\n",
            "Name: nist2, Length: 3997, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# Feature 15 extraction - \"NIST2\", similarity of two sentences\n",
        "train_data['nist2'] = FeatureExtraction_NIST2(train_data['sentence1'], train_data['sentence2'])\n",
        "dev_data['nist2'] = FeatureExtraction_NIST2(dev_data['sentence1'],dev_data['sentence2'])\n",
        "test_data['nist2'] = FeatureExtraction_NIST2(test_data['sentence1'], test_data['sentence2'])\n",
        "print('\\n Feature 15 - NIST2 extracted')\n",
        "print(dev_data['nist2'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "frRduoHP799L",
        "outputId": "81c0868d-249e-41d3-8a47-d820c4f675fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Feature 16 - NIST3 extracted\n",
            "0       0.163180\n",
            "1       0.275646\n",
            "2       0.123285\n",
            "3       0.134915\n",
            "4       0.228099\n",
            "          ...   \n",
            "3992    0.317667\n",
            "3993    0.348143\n",
            "3994    0.196662\n",
            "3995    0.226528\n",
            "3996    0.164343\n",
            "Name: nist3, Length: 3997, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# Feature 16 extraction - \"NIST3\", similarity of two sentences\n",
        "train_data['nist3'] = FeatureExtraction_NIST3(train_data['sentence1'], train_data['sentence2'])\n",
        "dev_data['nist3'] = FeatureExtraction_NIST3(dev_data['sentence1'],dev_data['sentence2'])\n",
        "test_data['nist3'] = FeatureExtraction_NIST3(test_data['sentence1'], test_data['sentence2'])\n",
        "print('\\n Feature 16 - NIST3 extracted')\n",
        "print(dev_data['nist3'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXxrC85U8AaZ",
        "outputId": "fb807376-c37a-42d4-b14a-395cb8b0d20e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Feature 18 - meteor extracted\n",
            "0       0.946609\n",
            "1       0.077320\n",
            "2       0.925241\n",
            "3       0.895514\n",
            "4       0.815090\n",
            "          ...   \n",
            "3992    0.119863\n",
            "3993    0.638812\n",
            "3994    0.926762\n",
            "3995    0.678389\n",
            "3996    0.904696\n",
            "Name: meteor, Length: 3997, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# Feature 18 extraction - \"METEOR\", similarity of two sentences\n",
        "train_data['meteor'] = FeatureExtraction_Meteor(train_data['sentence1'], train_data['sentence2'])\n",
        "dev_data['meteor'] = FeatureExtraction_Meteor(dev_data['sentence1'],dev_data['sentence2'])\n",
        "#d_train['meteor'] = FeatureExtraction_Meteor(d_train['tok_sentence1'],d_train['tok_sentence2'])\n",
        "test_data['meteor'] = FeatureExtraction_Meteor(test_data['sentence1'], test_data['sentence2'])\n",
        "print('\\n Feature 18 - meteor extracted')\n",
        "print(dev_data['meteor'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OL9-cYHU8BxX",
        "outputId": "04354f4a-6efe-4a9f-9f7a-e5d7905ad8bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Feature 19 - LD extracted\n",
            "0        64\n",
            "1       107\n",
            "2        92\n",
            "3       138\n",
            "4        51\n",
            "       ... \n",
            "3992    111\n",
            "3993     55\n",
            "3994     91\n",
            "3995     92\n",
            "3996     94\n",
            "Name: ld, Length: 3997, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Feature 19 extraction - \"LD\", similarity of two sentences\n",
        "train_data['ld'] = FeatureExtraction_LD(train_data['sentence1'], train_data['sentence2'])\n",
        "dev_data['ld'] = FeatureExtraction_LD(dev_data['sentence1'],dev_data['sentence2'])\n",
        "#d_train['ld'] = FeatureExtraction_LD(d_train['tok_sentence1'],d_train['tok_sentence2'])\n",
        "test_data['ld'] = FeatureExtraction_LD(test_data['sentence1'], test_data['sentence2'])\n",
        "print('\\n Feature 19 - LD extracted')\n",
        "print(dev_data['ld'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ifYowA4G2LI5",
        "outputId": "d2069b8b-1827-44bc-aeab-73ac9da3d8f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 19 Vector labels are : 1-word_count_dif, 2-word_ovrlap, 3-word_union, 4-3gram_overlap, 5-3gram_union, 6-2gram_overlap, 7-2gram_union, 8-num_overlap, 9-cap_words_overlap, 10-cosine_sim \n"
          ]
        }
      ],
      "source": [
        "# Vectors\n",
        "print(\" 19 Vector labels are : 1-word_count_dif, 2-word_ovrlap, 3-word_union, 4-3gram_overlap, 5-3gram_union, 6-2gram_overlap, 7-2gram_union, 8-num_overlap, 9-cap_words_overlap, 10-cosine_sim \")\n",
        "#train_data[['sentence1','sentence2','gold_label','word_count_dif','word_overlap','word_union','3gram_overlap','3gram_union','2gram_overlap','2gram_union','num_overlap','cap_words_overlap','cosine_sim']].head()\n",
        "#dev_data[['sentence1','sentence2','gold_label','word_count_dif','word_overlap','word_union','3gram_overlap','3gram_union','2gram_overlap','2gram_union','num_overlap','cap_words_overlap','cosine_sim']].head()\n",
        "#d_train[['sentence1','sentence2','gold_label','word_count_dif','word_overlap','word_union','3gram_overlap','3gram_union','2gram_overlap','2gram_union','num_overlap','cap_words_overlap','cosine_sim']].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8VRShJxWXAZ"
      },
      "source": [
        "# Data Selection for Training Section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H2RiLqNWizxb",
        "outputId": "43c5f9cf-d921-4988-ec73-dc94dd91227f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tr_vect shape:  (7799, 17) \n",
            "tr_class shape:  (7799,)\n",
            "dev_vect shape:  (3997, 17) \n",
            "dev_class shape:  (3997,)\n",
            "\n",
            " train_data ready for training\n"
          ]
        }
      ],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt # plotting \n",
        "\n",
        "# Training vectors and classes\n",
        "test_vect = test_data[['word_count_dif','word_overlap','word_union','3gram_overlap','3gram_union','2gram_overlap','2gram_union','num_overlap','cap_words_overlap','cosine_sim', 'bleu4', 'bleu2', 'bleu3', 'nist2', 'nist3', 'meteor', 'ld']].astype(float)\n",
        "\n",
        "# Training vectors and classes\n",
        "tr_vect = train_data[['word_count_dif','word_overlap','word_union','3gram_overlap','3gram_union','2gram_overlap','2gram_union','num_overlap','cap_words_overlap','cosine_sim', 'bleu4', 'bleu2', 'bleu3', 'nist2', 'nist3', 'meteor', 'ld']].astype(float)\n",
        "tr_class = train_data['gold_label'].astype(int)\n",
        "\n",
        "#tr_class = tr_class[:700]\n",
        "print('tr_vect shape: ',tr_vect.shape,'\\ntr_class shape: ',tr_class.shape )\n",
        "\n",
        "# Test vectors and classes\n",
        "dev_vect = dev_data[['word_count_dif','word_overlap','word_union','3gram_overlap','3gram_union','2gram_overlap','2gram_union','num_overlap','cap_words_overlap','cosine_sim', 'bleu4', 'bleu2', 'bleu3', 'nist2', 'nist3', 'meteor', 'ld']].astype(float)\n",
        "dev_class = dev_data['gold_label'].astype(int)\n",
        "print('dev_vect shape: ',dev_vect.shape,'\\ndev_class shape: ',dev_class.shape )\n",
        "\n",
        "print('\\n train_data ready for training')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1iac1_BtUluY"
      },
      "source": [
        "# MLP Section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4xSfGG0d_xt6",
        "outputId": "2a213620-89d5-41e4-b94b-cadbfc9f8aec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[5540  359]\n",
            " [ 219 1681]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.94      0.95      5899\n",
            "           1       0.82      0.88      0.85      1900\n",
            "\n",
            "    accuracy                           0.93      7799\n",
            "   macro avg       0.89      0.91      0.90      7799\n",
            "weighted avg       0.93      0.93      0.93      7799\n",
            "\n",
            "       instance_id  predicted_label\n",
            "0        test_id_0                0\n",
            "1        test_id_1                0\n",
            "2        test_id_2                1\n",
            "3        test_id_3                1\n",
            "4        test_id_4                0\n",
            "...            ...              ...\n",
            "3993  test_id_3995                0\n",
            "3994  test_id_3996                0\n",
            "3995  test_id_3997                0\n",
            "3996  test_id_3998                0\n",
            "3997  test_id_3999                0\n",
            "\n",
            "[3998 rows x 2 columns]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:549: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
          ]
        }
      ],
      "source": [
        "# Multi Layer Perceptron \n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
        "from sklearn.preprocessing import StandardScaler  \n",
        "import numpy as np\n",
        "\n",
        "scaler = StandardScaler()  \n",
        "\n",
        "scaler.fit(tr_vect)  \n",
        "tr_vect = scaler.transform(tr_vect)  \n",
        "\n",
        "scaler.fit(dev_vect)\n",
        "dev_vect = scaler.transform(dev_vect)  \n",
        "\n",
        "scaler.fit(test_vect)  \n",
        "test_vect = scaler.transform(test_vect) \n",
        "\n",
        "#GridSearchCV \n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "#clf = MLPClassifier(activation='logistic', learning_rate='adaptive', random_state=1, max_iter=300).fit(tr_vect, tr_class)\n",
        "#y_pred = clf.predict(dev_vect)\n",
        "\n",
        "#mlp = MLPClassifier(hidden_layer_sizes=(17,), activation='relu', solver='adam', warm_start=True, max_iter=500)\n",
        "parameters = {'solver': ['lbfgs'], 'alpha': 10.0 ** -np.arange(1, 10), 'hidden_layer_sizes':np.arange(10, 17), 'random_state':[0,1,2,3,4,5,6,7,8,9]}\n",
        "#clf = GridSearchCV(MLPClassifier(), parameters, n_jobs=-1)\n",
        "#clf.fit(tr_vect, tr_class)\n",
        "\n",
        "#print(mlp.best_params_)\n",
        "\n",
        "clf = MLPClassifier(activation='logistic', alpha = 0.1, hidden_layer_sizes = 13, learning_rate='adaptive', random_state=4, max_iter=100, solver = 'lbfgs').fit(tr_vect, tr_class)\n",
        "predict_train = clf.predict(tr_vect)\n",
        "predict_test = clf.predict(test_vect)\n",
        "\n",
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "print(confusion_matrix(tr_class,predict_train))\n",
        "print(classification_report(tr_class,predict_train))\n",
        "\n",
        "# test on dev\n",
        "#print(confusion_matrix(dev_class,predict_test))\n",
        "#print(classification_report(dev_class,predict_test))\n",
        "#print(f1_score(dev_class, predict_test)) \n",
        "\n",
        "prediction = pd.DataFrame()\n",
        "prediction['instance_id'] = test_data['id']\n",
        "prediction['predicted_label'] = predict_test\n",
        "print(prediction)\n",
        "prediction.to_csv('DariiaDragunova_test_result.txt', sep ='\\t', header = None, index = None)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "lxtNes4y72qU",
        "bsNpvZxyTYaJ",
        "z8VRShJxWXAZ"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}